## Large Language Models (LLM)

### Models

[<img src="../assets/github.png" width="20" />]()
[<img src="../assets/github.png" width="20" />](https://github.com/tatsu-lab/stanford_alpaca)
[ðŸ¤—]()

|   Date    |     Name     | Publication | Repositories |
| :-------: | :---------- | :--------- | :---------: |
| `` |  | []() |  |
| `` |  | []() |  |
| `` |  | []() |  |
| `2024.01` | Mixtral | [Mixtral of Experts](https://arxiv.org/abs/2401.04088) |  |
| `2023.10` | Mistral | [Mistral 7B](https://arxiv.org/abs/2310.06825) |  |
| `2023.09` | NExT-GPT | [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519) | [<img src="../assets/github.png" width="20" />](https://github.com/NExT-GPT/NExT-GPT) |
| `2023.07` | LLaMa2 | [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) |  |
| `2023.06` | Macaw | [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093) | [<img src="../assets/github.png" width="20" />](https://github.com/lyuchenyang/Macaw-LLM) |
| `2023.05` | PaLM 2 | [PaLM 2 Technical Report](https://arxiv.org/abs/2305.10403) |  |
| `2023.03` | Alpaca | [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html) | [<img src="../assets/github.png" width="20" />](https://github.com/tatsu-lab/stanford_alpaca) |
| `2023.02` | LLaMa   | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) |  |
| `2022.10` | U-PaLM | [Transcending Scaling Laws with 0.1% Extra Compute](https://arxiv.org/abs/2210.11399) |  |
| `2022.04` | PaLM | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) |  |
| `2021.07` | CODEX   | [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374) |  |
| `2020.06` | DeBERTa | [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) |  |
| `2019.10` | BART | [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) |  |
| `2019.10` | T5 | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) |  |
| `2019.09` | ALBERT | [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) |  |
| `2019.09` | Megatron | [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) |  |
| `2019.07` | RoBERTa | [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) |  |
| `2019.06` | XLNet | [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) |  |
| `2019.02` | GPT-2 | [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [<img src="../assets/github.png" width="20" />](https://github.com/openai/gpt-2) |
| `2018.10` | BERT | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) |  |
| `2018.06` | GPT | [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) |  |
| `2017.06` | Transformers | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) |  |

### Reviews

|   Date    |     Name     | Publication | Repositories |
| :-------: | :---------- | :--------- | :---------: |
| `2024.02` |  | [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196) | |
| `2024.02` |  | [A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges](https://ieeexplore.ieee.org/document/10433480) | |
| `2023.10` |  | [Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2310.19736) | |
| `2023.09` |  | [Large Language Model Alignment: A Survey](https://arxiv.org/abs/2309.15025) |  |
| `2023.07` |  | [A Comprehensive Overview of Large Language Models](https://arxiv.org/abs/2307.06435) | |
| `2023.07` |  | [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109) | |
| `2023.03` |  | [Survey of Hallucination in Natural Language Generation](https://dl.acm.org/doi/10.1145/3571730) | |
| `2023.02` |  | [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842) | |

### Miscelaneous

|   Date    |     Name      | Publication | Repositories |
| :-------: | :----------   | :--------- | :---------: |
|  | Knowledge-Graph LLM Papers | [KG-LLM-Papers]() | [<img src="../assets/github.png" width="20" />](https://github.com/zjukg/KG-LLM-Papers) |

