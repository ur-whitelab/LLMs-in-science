## Large Language Models (LLM)

### Models

<!-- 
[<img src="../assets/github.png" width="20" />]()
[ü§ó]() 
[üåê]()
-->


|   Date    |     Name     | Publication | Repositories |
| :-------: | :---------- | :--------- | :---------: |
| `2024.04` | Phi-3         | [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219v3) |  |
| `2024.03` | Claude 3      | [The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/f2986af8d052f26236f6251da62d16172cfabd6e/claude-3-model-card.pdf) |  [üåê](https://www.anthropic.com/news/claude-3-family) |
| `2024.01` | Mixtral       | [Mixtral of Experts](https://arxiv.org/abs/2401.04088) |  |
| `2023.12` | Gemini        | [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) |  |
| `2023.12` | Phi-2         | [Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) | [ü§ó](https://huggingface.co/microsoft/phi-2) |
| `2023.11` | Falcon        | [The Falcon Series of Open Language Models](https://arxiv.org/abs/2311.16867) |  |
| `2023.10` | Mistral       | [Mistral 7B](https://arxiv.org/abs/2310.06825) |  |
| `2023.10` | Zephyr        | [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944) |  |
| `2023.09` | Phi-1.5       | [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) |  |
| `2023.09` | NExT-GPT      | [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519) | [<img src="../assets/github.png" width="20" />](https://github.com/NExT-GPT/NExT-GPT) |
| `2023.07` | Claude 2      | [Model Card and Evaluations for Claude Models](https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf) |  [üåê](https://www.anthropic.com/news/claude-2) |
| `2023.07` | LLaMa2        | [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) |  |
| `2023.06` | Macaw         | [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093) | [<img src="../assets/github.png" width="20" />](https://github.com/lyuchenyang/Macaw-LLM) |
| `2023.06` | Phi-1         | [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) |  |
| `2023.05` | PaLM 2        | [PaLM 2 Technical Report](https://arxiv.org/abs/2305.10403) |  |
| `2023.05` | Gorilla       | [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334) | [üåê](https://gorilla.cs.berkeley.edu/), [<img src="../assets/github.png" width="20" />](https://github.com/ShishirPatil/gorilla) [ü§ó](https://huggingface.co/gorilla-llm) |
| `2023.04` | Koala         | [Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/) |  |
| `2023.03` | Vicuna        | [Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality](https://lmsys.org/blog/2023-03-30-vicuna/) | [<img src="../assets/github.png" width="20" />](https://github.com/lm-sys/FastChat) |
| `2023.03` | Alpaca        | [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html) | [<img src="../assets/github.png" width="20" />](https://github.com/tatsu-lab/stanford_alpaca) |
| `2023.02` | LLaMa         | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) |  |
| `2022.11` | Galactica     | [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085) |  |
| `2022.10` | Flan-T5       | [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) | [<img src="../assets/github.png" width="20" />](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) |
| `2022.10` | U-PaLM        | [Transcending Scaling Laws with 0.1% Extra Compute](https://arxiv.org/abs/2210.11399) |  |
| `2022.04` | PaLM          | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) |  |
| `2022.03` | Chinchilla    | [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) |  |
| `2021.12` | Gopher        | [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446) |  |
| `2021.07` | CODEX         | [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374) |  |
| `2020.06` | DeBERTa       | [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) |  |
| `2019.10` | BART          | [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) |  |
| `2019.10` | T5            | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) |  |
| `2019.09` | ALBERT        | [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) |  |
| `2019.09` | Megatron      | [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) |  |
| `2019.07` | RoBERTa       | [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) |  |
| `2019.06` | XLNet         | [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) |  |
| `2019.02` | GPT-2         | [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [<img src="../assets/github.png" width="20" />](https://github.com/openai/gpt-2) |
| `2018.10` | BERT          | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) |  |
| `2018.06` | GPT           | [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) |  |
| `2017.06` | Transformers  | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) |  |

### Reviews

|   Date    |     Name     | Publication | Repositories |
| :-------: | :---------- | :--------- | :---------: |
| `2024.02` |  | [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196) | |
| `2024.02` |  | [A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges](https://ieeexplore.ieee.org/document/10433480) | |
| `2023.10` |  | [Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2310.19736) | [<img src="../assets/github.png" width="20" />](https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers) |
| `2023.09` |  | [Large Language Model Alignment: A Survey](https://arxiv.org/abs/2309.15025) |  |
| `2023.07` |  | [A Comprehensive Overview of Large Language Models](https://arxiv.org/abs/2307.06435) |  |
| `2023.07` |  | [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109) | [<img src="../assets/github.png" width="20" />](https://github.com/MLGroupJLU/LLM-eval-survey) |
| `2023.03` |  | [Survey of Hallucination in Natural Language Generation](https://dl.acm.org/doi/10.1145/3571730) |  |
| `2023.02` |  | [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842) | [<img src="../assets/github.png" width="20" />]() |



### Miscellaneous

|   Date    |     Name      | Publication | Repositories |
| :-------: | :----------   | :--------- | :---------: |
|  | Knowledge-Graph LLM Papers | [KG-LLM-Papers]() | [<img src="../assets/github.png" width="20" />](https://github.com/zjukg/KG-LLM-Papers) |

