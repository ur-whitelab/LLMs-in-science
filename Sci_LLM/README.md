## LLMs in science
### Models

|   Date    |     Name        | Publication | Repositories |
| :-------: | :-------------- | :---------- | :----------: |
| `2024.10` | ChemLML         | [Chemical Language Model Linker: blending text and molecules with modular adapters](https://arxiv.org/abs/2410.20182) | [<img src="../assets/github.png" width="20" />](https://github.com/gitter-lab/ChemLML) |
| `2024.06` | MatText        | [MatText: Do Language Models Need More than Text & Scale for Materials Modeling?](https://arxiv.org/abs/2406.17295) | [<img src="../assets/github.png" width="20" />](https://github.com/lamalab-org/mattext) |
| `2024.06` | BioLunar        | [An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery](https://arxiv.org/abs/2406.18626) | [<img src="../assets/github.png" width="20" />](https://github.com/neuro-symbolic-ai/lunar-bioverse-demo) |
| `2024.05` |         | [Specialising and Analysing Instruction-Tuned and Byte-Level Language Models for Organic Reaction Prediction](https://arxiv.org/abs/2405.10625) | [<img src="../assets/github.png" width="20" />]() |
| `2024.04` | DockingGA        | [DockingGA: enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation](https://academic.oup.com/bfg/advance-article-abstract/doi/10.1093/bfgp/elae011/7641743?redirectedFrom=fulltext&login=true) | [<img src="../assets/github.png" width="20" />]() |
| `2024.03` | BioMedLM        | [BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text](https://arxiv.org/abs/2403.18421) | [ðŸ¤—](https://huggingface.co/stanford-crfm/BioMedLM) |
| `2024.03` | TSMMG           | [Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model](https://arxiv.org/abs/2403.13244) |  |
| `2024.03` | 3M-Diffusion    | [3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs](https://arxiv.org/abs/2403.07179) | [<img src="../assets/github.png" width="20" />](https://github.com/huaishengzhu/3MDiffusion) |
| `2024.03` | MediSwift       | [MediSwift: Efficient Sparse Pre-trained Biomedical Language Models](https://arxiv.org/abs/2403.00952) |  |
| `2024.02` | BioT5+          | [BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning](https://arxiv.org/abs/2402.17810) | [<img src="../assets/github.png" width="20" />](https://github.com/QizhiPei/BioT5) |
| `2024.02` | BiMediX         | [BiMediX: Bilingual Medical Mixture of Experts LLM](https://arxiv.org/abs/2402.13253) | [<img src="../assets/github.png" width="20" />](https://github.com/mbzuai-oryx/BiMediX) |
| `2024.02` | TGM-DLM         | [Text-Guided Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2402.13040) |  |
| `2024.02` | Transformer-VAE | [A novel molecule generative model of VAE combined with Transformer for unseen structure generation](https://arxiv.org/abs/2402.11950) |  |
| `2024.02` | BioMistral      | [BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains](https://arxiv.org/abs/2402.10373) | [ðŸ¤—](https://huggingface.co/BioMistral) |
| `2024.02` | GPTChem         | [Leveraging Large Language Models for Predictive Chemistry](https://chemrxiv.org/engage/chemrxiv/article-details/652e50b98bab5d2055852dde) | [<img src="../assets/github.png" width="20" />](https://github.com/lamalab-org/chemlift) |
| `2024.02` | MolTC           | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) |  |
| `2024.01` | GIT-Mol         | [GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text](https://www.sciencedirect.com/science/article/pii/S0010482524001574?via%3Dihub) |  |
| `2024.01` | AutoMolDesigner | [AutoMolDesigner for Antibiotic Discovery: An AI-Based Open-Source Software for Automated Design of Small-Molecule Antibiotics](https://pubs.acs.org/doi/10.1021/acs.jcim.3c01562) | [<img src="../assets/github.png" width="20" />](https://github.com/taoshen99/AutoMolDesigner) |
| `2024.01` | CheXagent       | [CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2401.12208) | [<img src="../assets/github.png" width="20" />](https://stanford-aimi.github.io/chexagent.html) |
| `2024.01` | CONSMI          | [CONSMI: Contrastive Learning in the Simplified Molecular Input Line Entry System Helps Generate Better Molecules](https://www.mdpi.com/1420-3049/29/2/495) |  |
| `2024.01` | INTransformer   | [INTransformer: Data augmentation-based contrastive learning by injecting noise into transformer for molecular property prediction](https://www.sciencedirect.com/science/article/pii/S1093326324000032) |  |
| `2023.12` | LLaVa-Med       | [LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5abcdf8ecdcacba028c6662789194572-Abstract-Datasets_and_Benchmarks.html) | [<img src="../assets/github.png" width="20" />](https://github.com/microsoft/LLaVA-Med) |
| `2023.12` | BioT5           | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations](https://aclanthology.org/2023.emnlp-main.70/) | [<img src="../assets/github.png" width="20" />](https://github.com/QizhiPei/BioT5) |
| `2023.11` | ReactionT5      | [ReactionT5: a large-scale pre-trained model towards application of limited reaction data](https://arxiv.org/abs/2311.06708) |  |
| `2023.11` | CatBERTa        | [Catalyst Energy Prediction with CatBERTa: Unveiling Feature Exploration Strategies through Large Language Models](https://pubs.acs.org/doi/10.1021/acscatal.3c04956) | [<img src="../assets/github.png" width="20" />](https://github.com/hoon-ock/CatBERTa) |
| `2023.10` | PremuNet        | [A pre-trained multi-representation fusion network for molecular property prediction](https://www.sciencedirect.com/science/article/pii/S1566253523004086?via%3Dihub)|  |
| `2023.10` | T-MGCL          | [T-MGCL: Molecule Graph Contrastive Learning Based on Transformer for Molecular Property Prediction](https://ieeexplore.ieee.org/document/10288021)|  |
| `2023.10` | MatGPT          | [MatGPT: A Vane of Materials Informatics from Past, Present, to Future](https://onlinelibrary.wiley.com/doi/full/10.1002/adma.202306733)|  |
| `2023.09` | ChemSpaceAL     | [ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation](https://pubs.acs.org/doi/10.1021/acs.jcim.3c01456?ref=PDF)|  |
| `2023.08` | DARWIN          | [DARWIN Series: Domain Specific Large Language Models for Natural Science](https://arxiv.org/abs/2308.13565)| [<img src="../assets/github.png" width="20" />](https://github.com/MasterAI-EAM/Darwin) |
| `2023.08` | BioMedGPT    | [BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine](https://arxiv.org/abs/2308.09442)| [<img src="../assets/github.png" width="20" />](https://github.com/PharMolix/OpenBioMed.) |
| `2023.06` |                 | [Generative Pre-trained Transformer (GPT) based model with relative attention for de novo drug design](https://www.sciencedirect.com/science/article/pii/S1476927123001020?via%3Dihub)|  |
| `2023.06` | SGPT-RL         | [Optimization of binding affinities in chemical space with generative pre-trained transformer and deep reinforcement learning](https://f1000research.com/articles/12-757/v2)|  |
| `2023.06` | TransAntivirus  | [Transformer-Based Molecular Generative Model for Antiviral Drug Design](https://pubs.acs.org/doi/10.1021/acs.jcim.3c00536)|  |
| `2023.06` | MULTIMODAL-MOLFORMER  | [Beyond Chemical Language: A Multimodal Approach to Enhance Molecular Property Prediction](http://arxiv.org/abs/2306.14919)|  |
| `2023.05` | Taiga           | [Molecule generation using transformers and policy gradient reinforcement learning](https://www.nature.com/articles/s41598-023-35648-w)|  |
| `2023.05` | ChatDrug        | [ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback](https://arxiv.org/abs/2305.18090)| [<img src="../assets/github.png" width="20" />](https://github.com/chao1224/ChatDrug) |
| `2023.05` | MolXPT          | [MolXPT: Wrapping Molecules with Text for Generative Pre-training](https://arxiv.org/abs/2305.10688)|  |
| `2023.05` | Text+Chem T5    | [Unifying Molecular and Textual Representations via Multi-task Language Modelling](https://arxiv.org/abs/2301.12586)| [<img src="../assets/github.png" width="20" />](https://github.com/GT4SD/multitask_text_and_chemistry_t5)[ðŸ¤—](https://huggingface.co/spaces/GT4SD/multitask-text-and-chemistry-t5) |
| `2023.05` | iupacGPT        | [iupacGPT: IUPAC-based large-scale molecular pre-trained model for property prediction and molecule generation](https://chemrxiv.org/engage/chemrxiv/article-details/645f49f9a32ceeff2d90c9ae)|  |
| `2023.04` | MaterialsBERT   | [A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing](https://www.nature.com/articles/s41524-023-01003-w) | |
| `2023.04` | PMC-LLaMA       | [PMC-LLaMA: Towards Building Open-source Language Models for Medicine](https://arxiv.org/abs/2304.14454)| [<img src="../assets/github.png" width="20" />](https://github.com/chaoyi-wu/PMC-LLaMA) |
| `2023.04` | cMolGPT         | [cMolGPT: A Conditional Generative Pre-Trained Transformer for Target-Specific De Novo Molecular Generation](https://www.mdpi.com/1420-3049/28/11/4430)| [<img src="../assets/github.png" width="20" />](https://github.com/VV123/cMolGPT) |
| `2023.04` | TransPolymer    | [TransPolymer: a Transformer-based language model for polymer property predictions](https://www.nature.com/articles/s41524-023-01016-5)| [<img src="../assets/github.png" width="20" />](https://github.com/ChangwenXu98/TransPolymer) |
| `2023.04` | MedAlpaca       | [MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data](https://arxiv.org/abs/2304.08247)|  |
| `2023.04` | SELFormer       | [SELFormer: Molecular Representation Learning via SELFIES Language Models](https://arxiv.org/abs/2304.04662)|  |
| `2023.04` | Regression Transformer | [Regression Transformer enables concurrent sequence regression and generation for molecular language modelling](https://www.nature.com/articles/s42256-023-00639-z)| [ðŸ¤—](https://huggingface.co/pranav-s/MaterialsBERT) |
| `2023.04` | MaterialsBERT   | [A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing](https://www.nature.com/articles/s41524-023-01003-w)|  |
| `2023.03` | ChatDoctor      | [ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge](https://arxiv.org/abs/2303.14070)|  |
| `2023.03` | CLAMP           | [Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language](https://arxiv.org/abs/2303.03363)|  |
| `2023.02` |                 | [Enhancing diversity in language based models for single-step retrosynthesis ](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00110a)|  |
| `2023.02` | BioTranslator   | [Multilingual translation for zero-shot biomedical classification using BioTranslator](https://www.nature.com/articles/s41467-023-36476-2)| [<img src="../assets/github.png" width="20" />](https://github.com/HanwenXuTHU/BioTranslatorProject) |
| `2023.01` | MOLGEN          | [Domain-Agnostic Molecular Generation with Chemical Feedback](https://arxiv.org/abs/2301.11259)|  |
| `2023.01` | CRTMorgan       | [Explore drug-like space with deep generative models](https://www.sciencedirect.com/science/article/pii/S1046202323000129?via%3Dihub)|  |
| `2023.01` | PeTrans         | [PETrans: De Novo Drug Design with Protein-Specific Encoding Based on Transfer Learning](https://www.mdpi.com/1422-0067/24/2/1146)|  |
| `2022.11` | Galactica       | [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)|  |
| `2022.10` |                 | [Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-022-00646-z)|  |
| `2022.10` | SolvBERT        | [SolvBERT for solvation free energy and solubility prediction: a demonstration of an NLP model for predicting the properties of molecular complexes](https://chemrxiv.org/engage/chemrxiv/article-details/633d6bbfea6a225b1809e24e) | |
| `2022.09` | BioGPT          | [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/article/23/6/bbac409/6713511?login=true)| [<img src="../assets/github.png" width="20" />](https://github.com/microsoft/BioGPT) |
| `2022.09` | ChemBERTa-2     | [ChemBERTa-2: Towards Chemical Foundation Models](https://arxiv.org/abs/2209.01712) | |
| `2022.08` | AI-cell         | [Artificial Immune Cell, AI-cell, a New Tool to Predict Interferon Production by Peripheral Blood Monocytes in Response to Nucleic Acid Nanoparticles](https://onlinelibrary.wiley.com/doi/full/10.1002/smll.202204941)|  |
| `2022.05` | MatSciBERT      | [MatSciBERT: A materials domain language model for text mining and information extraction](https://www.nature.com/articles/s41524-022-00784-w) | |
| `2022.05` | MolFormer       | [Molformer: Large Scale Chemical Language Representations Capture Molecular Structure and Properties](https://www.researchsquare.com/article/rs-1570270/v1) | |
| `2022.05` | ScholarBERT     | [The Diminishing Returns of Masked Language Models to Science](https://arxiv.org/abs/2205.11342) | |
| `2022.04` | MolT5           | [Translation between Molecules and Natural Language](https://arxiv.org/abs/2204.11817)|  |
| `2022.04` | MatBERT         | [Molecular representation learning with language models and domain-relevant auxiliary tasks](https://arxiv.org/abs/2011.13230) | |
| `2022.03` | T5Chem          | [Unified Deep Learning Model for Multitask Reaction Predictions with Explanation](https://pubs.acs.org/doi/10.1021/acs.jcim.1c01467)| [<img src="../assets/github.png" width="20" />](https://yzhang.hpc.nyu.edu/T5Chem/) |
| `2022.01` | ChemFormer      | [Chemformer: a pre-trained transformer for computational chemistry](https://iopscience.iop.org/article/10.1088/2632-2153/ac3ffb)| [<img src="../assets/github.png" width="20" />](https://github.com/MolecularAI/Chemformer) |
| `2021.11` | Text2Mol        | [Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries](https://aclanthology.org/2021.emnlp-main.47/)|  |
| `2021.10` | MolGPT          | [MolGPT: Molecular Generation Using a Transformer-Decoder Model](https://pubs.acs.org/doi/full/10.1021/acs.jcim.1c00600)|  |
| `2021.10` | MTL-BERT        | [ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments](https://arxiv.org/abs/2110.03895) | |
| `2021.10` | PubMedBERT      | [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754) | |
| `2021.09` | SMILES-GPT      | [Generative Pre-Training from Molecules](https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e)| [<img src="../assets/github.png" width="20" />](https://github.com/sanjaradylov/smiles-gpt) |
| `2021.09` | Mol-BERT        | [Generative Adversarial Networks for Multi-Modal Multimedia Computing](https://www.hindawi.com/journals/wcmc/2021/7181815/) | |
| `2021.06` | ChemBERT        | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://pubs.acs.org/doi/10.1021/acs.jcim.1c00284) | |
| `2021.04` |                 | [Extraction of organic chemistry grammar from unsupervised learning of chemical reactions](https://www.science.org/doi/10.1126/sciadv.abe4166)|  |
| `2021.03` |                 | [Prediction of chemical reaction yields using deep learning](https://iopscience.iop.org/article/10.1088/2632-2153/abc81d/meta)|  |
| `2020.12` |                 | [Mapping the Space of Chemical Reactions Using Attention-Based Neural Networks](https://arxiv.org/abs/2012.06051) | |
| `2020.11` | MolBERT         | [Molecular representation learning with language models and domain-relevant auxiliary tasks](https://arxiv.org/abs/2011.13230) | |
| `2020.10` | Bio-Megatron    | [BioMegatron: Larger Biomedical Domain Language Model](https://arxiv.org/abs/2010.06060) | |
| `2020.10` | ChemBERTa       | [ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction](https://arxiv.org/abs/2010.09885) | |
| `2020.10` | MOFTransformers | [A Multi-modal Pre-training Transformer for Universal Transfer Learning in Metal-Organic Frameworks](https://chemrxiv.org/engage/chemrxiv/article-details/634fbf8a4a18764f58e9fda5)| [<img src="../assets/github.png" width="20" />](https://github.com/hspark1212/MOFTransformer) |
| `2020.04` | ClinicalBERT    | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) | |
| `2020.02` | BioBERT         | [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)| [<img src="../assets/github.png" width="20" />](https://github.com/naver/biobert-pretrained) |
| `2019.09` | SMILES-BERT     | [SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction](https://dl.acm.org/doi/10.1145/3307339.3342186) | |
| `2019.09` | SciBERT         | [SciBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676) | |
| `2019.08` | Molecular Transformer | [Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction](https://pubs.acs.org/doi/10.1021/acscentsci.9b00576)| [<img src="../assets/github.png" width="20" />]() |
| `2019.06` | BlueBERT        | [Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets](https://arxiv.org/abs/1906.05474) | |
| `2019.04` | ClinicalBERT    | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) | [ðŸ¤—](https://huggingface.co/medicalai/ClinicalBERT) |

### Reviews
|   Date    |     Name     | Publication | Repositories |
| :-------: | :---------- | :--------- | :---------: |
| `2024.02` |  | [A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation](https://arxiv.org/abs/2402.08703) | [<img src="../assets/github.png" width="20" />](https://github.com/gersteinlab/GenAI4Drug) |
| `2024.02` |  | [Advances in machine learning with chemical language models in molecular property and reaction outcome predictions](https://onlinelibrary.wiley.com/doi/10.1002/jcc.27315) | |
| `2024.01` |  | [Scientific Large Language Models: A Survey on Biological & Chemical Domains](https://arxiv.org/abs/2401.14656) | |
| `2024.01` |  | [The Future of Molecular Studies through the Lens of Large Language Models](https://pubs.acs.org/doi/10.1021/acs.jcim.3c01977) | |
| `2023.08` |  | [Pre-trained language models in medicine: A survey](https://www.sciencedirect.com/science/article/pii/S0933365724001465?via%3Dihub) | |
| `2023.05` |  | [What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks](https://arxiv.org/abs/2305.18365) | |
| `2023.04` |  | [Chemical language models for de novo drug design: Challenges and opportunities](https://www.sciencedirect.com/science/article/pii/S0959440X23000015?via%3Dihub) | |
| `2023.03` |  | [The future of chemistry is language](https://www.nature.com/articles/s41570-023-00502-0) | |
| `2023.01` |  | [Assessment of chemistry knowledge in large language models that generate code](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00087c) | |

### Reviews
|   Date    |     Name      | Publication | Repositories |
| :-------: | :----------   | :--------- | :---------: |
|  |  | [Scientific Large Language Models: A Survey on Biological & Chemical Domains](https://arxiv.org/abs/2401.14656) | [<img src="../assets/github.png" width="20" />](https://github.com/HICAI-ZJU/Scientific-LLM-Survey) |


### Miscelaneous

|   Date    |     Name      | Publication | Repositories |
| :-------: | :----------   | :--------- | :---------: |
|  | Mozi | [Mozi: A Scientific Large-scale Language Model](https://github.com/gmftbyGMFTBY/science-llm) | [<img src="../assets/github.png" width="20" />](https://github.com/gmftbyGMFTBY/science-llm) |
|  |  | [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) | [<img src="../assets/github.png" width="20" />](https://github.com/Hannibal046/Awesome-LLM) |
|  |  | [Awesome Scientific Language Models](https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models) | [<img src="../assets/github.png" width="20" />](https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models) |
|  |  | [Generating scientific hypotheses with Large Language Models](https://github.com/Paureel/LLM-SCI-GEN) | [<img src="../assets/github.png" width="20" />](https://github.com/Paureel/LLM-SCI-GEN) |
